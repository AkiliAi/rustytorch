// rustytorch_examples/src/advanced_linalg.rs
// DÃ©monstration des opÃ©rations d'algÃ¨bre linÃ©aire avancÃ©es

use rustytorch_core::Reshapable;
use rustytorch_tensor::Tensor;

pub fn run_advanced_linalg_demo() {
    println!("ðŸ§® DÃ©monstration d'algÃ¨bre linÃ©aire avancÃ©e RustyTorch\n");

    // Test tensordot - produit tensoriel gÃ©nÃ©ralisÃ©
    println!("ðŸ”€ Test tensordot:");
    let matrix_a = Tensor::from_data(&[1.0f64, 2.0, 3.0, 4.0], vec![2, 2], None);
    let matrix_b = Tensor::from_data(&[5.0f64, 6.0, 7.0, 8.0], vec![2, 2], None);

    println!("Matrice A (2x2): {:?}", matrix_a.storage().to_vec_f64());
    println!("Matrice B (2x2): {:?}", matrix_b.storage().to_vec_f64());

    // tensordot avec axes (1,0) - Ã©quivalent Ã  la multiplication matricielle
    let tensordot_result = matrix_a.tensordot(&matrix_b, (vec![1], vec![0])).unwrap();
    println!(
        "Tensordot AâŠ—B axes([1],[0]): {:?}",
        tensordot_result.storage().to_vec_f64()
    );
    println!("Shape: {:?}", tensordot_result.shape());
    // Attendu: matmul(A, B) = [[1*5+2*7, 1*6+2*8], [3*5+4*7, 3*6+4*8]] = [[19, 22], [43, 50]]

    // Test outer product - produit extÃ©rieur
    println!("\nâŠ— Test outer product:");
    let vec_u = Tensor::from_data(&[1.0f64, 2.0, 3.0], vec![3], None);
    let vec_v = Tensor::from_data(&[4.0f64, 5.0], vec![2], None);

    println!("Vecteur u: {:?}", vec_u.storage().to_vec_f64());
    println!("Vecteur v: {:?}", vec_v.storage().to_vec_f64());

    let outer_result = vec_u.outer(&vec_v).unwrap();
    println!("Outer product uâŠ—v:");
    print_2d_tensor(&outer_result);
    // Attendu: [[1*4, 1*5], [2*4, 2*5], [3*4, 3*5]] = [[4, 5], [8, 10], [12, 15]]

    // Test diagonal extraction
    println!("\nðŸ“ Test diagonal extraction:");
    let matrix_c = Tensor::from_data(
        &[1.0f64, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0],
        vec![3, 3],
        None,
    );

    println!("Matrice C (3x3):");
    print_2d_tensor(&matrix_c);

    // Diagonale principale
    let main_diag = matrix_c.diagonal(0, None, None).unwrap();
    println!(
        "Diagonale principale: {:?}",
        main_diag.storage().to_vec_f64()
    );
    // Attendu: [1, 5, 9]

    // Diagonale supÃ©rieure (offset +1)
    let upper_diag = matrix_c.diagonal(1, None, None).unwrap();
    println!(
        "Diagonale supÃ©rieure (+1): {:?}",
        upper_diag.storage().to_vec_f64()
    );
    // Attendu: [2, 6]

    // Diagonale infÃ©rieure (offset -1)
    let lower_diag = matrix_c.diagonal(-1, None, None).unwrap();
    println!(
        "Diagonale infÃ©rieure (-1): {:?}",
        lower_diag.storage().to_vec_f64()
    );
    // Attendu: [4, 8]

    // Test trace
    println!("\nðŸŽ¯ Test trace (somme de la diagonale):");
    let trace_c = matrix_c.trace().unwrap();
    println!("Trace de C: {}", trace_c);
    // Attendu: 1 + 5 + 9 = 15

    // Test avec matrice rectangulaire
    let rect_matrix = Tensor::from_data(
        &[
            1.0f64, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0,
        ],
        vec![3, 4],
        None,
    );

    println!("\nMatrice rectangulaire (3x4):");
    print_2d_tensor(&rect_matrix);

    let rect_diag = rect_matrix.diagonal(0, None, None).unwrap();
    println!("Diagonale: {:?}", rect_diag.storage().to_vec_f64());
    // Attendu: [1, 6, 11] (min(3,4) = 3 Ã©lÃ©ments)

    // Applications pratiques
    println!("\nðŸ§  Applications pratiques:");

    // 1. Calcul de la norme de Frobenius avec trace
    println!("â€¢ Calcul de norme:");
    let small_matrix = Tensor::from_data(&[3.0f64, 4.0, 0.0, 0.0], vec![2, 2], None);

    // A^T @ A pour obtenir la matrice de Gram
    let at = small_matrix.transpose(0, 1).unwrap();
    let gram = at.matmul(&small_matrix).unwrap();
    let trace_gram = gram.trace().unwrap();
    let frobenius_norm = trace_gram.sqrt();

    println!("  Matrice: {:?}", small_matrix.storage().to_vec_f64());
    println!(
        "  Norme de Frobenius via trace(A^T@A): {:.3}",
        frobenius_norm
    );

    // 2. CrÃ©ation de matrice diagonale Ã  partir d'un vecteur
    println!("\nâ€¢ Construction de matrice diagonale:");
    let diag_values = vec![2.0f64, 3.0, 5.0];
    let identity = Tensor::from_data(
        &[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0],
        vec![3, 3],
        None,
    );

    // Simuler la crÃ©ation d'une matrice diagonale (en rÃ©alitÃ© on multiplierait chaque colonne)
    println!("  Valeurs diagonales: {:?}", diag_values);
    println!("  (Construction de matrice diagonale - Ã  implÃ©menter)");
    // 3. Produit vectoriel via outer product
    println!("â€¢ Produit de rang 1 via outer product:");
    let u = Tensor::from_data(&[1.0f64, 0.0], vec![2], None);
    let v = Tensor::from_data(&[0.0f64, 1.0], vec![2], None);
    let rank1 = u.outer(&v).unwrap();

    println!("  u = {:?}", u.storage().to_vec_f64());
    println!("  v = {:?}", v.storage().to_vec_f64());
    println!("  uâŠ—v (matrice de rang 1):");
    print_2d_tensor(&rank1);

    println!("\nâœ… DÃ©monstration d'algÃ¨bre linÃ©aire avancÃ©e terminÃ©e !");
    println!("ðŸ“¦ Nouvelles fonctionnalitÃ©s implÃ©mentÃ©es:");
    println!("   â€¢ tensordot() - Produit tensoriel gÃ©nÃ©ralisÃ©");
    println!("   â€¢ outer() - Produit extÃ©rieur de tenseurs");
    println!("   â€¢ diagonal() - Extraction de diagonales avec offset");
    println!("   â€¢ trace() - Somme des Ã©lÃ©ments diagonaux");
    println!("   â€¢ Support pour matrices rectangulaires et dÃ©calages");
}

/// Helper function to print 2D tensor in matrix format
fn print_2d_tensor(tensor: &Tensor) {
    let shape = tensor.shape();
    if shape.len() != 2 {
        println!("Cannot print non-2D tensor");
        return;
    }

    let data = tensor.storage().to_vec_f64();
    let rows = shape[0];
    let cols = shape[1];

    for r in 0..rows {
        print!("  [");
        for c in 0..cols {
            let val = data[r * cols + c];
            if c > 0 {
                print!(", ");
            }
            print!("{:5.1}", val);
        }
        println!("]");
    }
}
