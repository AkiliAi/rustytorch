// rustytorch_examples/src/device_demo.rs
// D√©monstration des fonctionnalit√©s device √©tendues

use rustytorch_core::{
    available_devices, cuda_is_available, current_device, device_count, metal_is_available,
    rocm_is_available, set_device, synchronize, Device, DeviceContext, DeviceManager,
};
use rustytorch_tensor::Tensor;

pub fn run_device_demo() {
    println!("üñ•Ô∏è  D√©monstration des fonctionnalit√©s Device √©tendues RustyTorch\n");

    // === Discovery des devices disponibles ===
    println!("üîç D√©couverte des devices:");

    // Afficher le device courant
    println!("Device courant: {}", current_device());

    // Lister tous les devices disponibles
    let devices = available_devices();
    println!("Devices disponibles: {:?}", devices);

    // V√©rifier la disponibilit√© de chaque type
    println!("\nüìä Disponibilit√© par type:");
    println!("  CUDA disponible: {}", cuda_is_available());
    println!("  Metal disponible: {}", metal_is_available());
    println!("  ROCm disponible: {}", rocm_is_available());

    // Compter les devices par type
    println!("\nüìà Nombre de devices par type:");
    println!("  GPUs CUDA: {}", device_count("cuda"));
    println!("  GPUs Metal: {}", device_count("metal"));
    println!("  GPUs ROCm: {}", device_count("rocm"));

    // === Test Device Manager ===
    println!("\nüéÆ Test Device Manager:");
    let manager = DeviceManager::new();

    // Obtenir les infos du CPU
    if let Some(cpu_info) = manager.get_device_info(&Device::Cpu) {
        println!("\nInfos CPU:");
        println!("  Nom: {}", cpu_info.name);
        println!("  Type: {:?}", cpu_info.device_type);
        println!(
            "  M√©moire totale: {} GB",
            cpu_info.total_memory / (1024 * 1024 * 1024)
        );
        println!(
            "  M√©moire disponible: {} GB",
            cpu_info.available_memory / (1024 * 1024 * 1024)
        );
    }

    // Simuler la d√©couverte de GPU si CUDA_VISIBLE_DEVICES est d√©fini
    if std::env::var("CUDA_VISIBLE_DEVICES").is_ok() {
        if let Some(cuda_info) = manager.get_device_info(&Device::Cuda(0)) {
            println!("\nInfos GPU CUDA:");
            println!("  Nom: {}", cuda_info.name);
            println!("  Type: {:?}", cuda_info.device_type);
            println!(
                "  M√©moire totale: {} GB",
                cuda_info.total_memory / (1024 * 1024 * 1024)
            );
            if let Some(cc) = cuda_info.compute_capability {
                println!("  Compute Capability: {}.{}", cc.major, cc.minor);
            }
        }
    }

    // === Test Device Context ===
    println!("\nüîß Test Device Context:");
    let context = DeviceContext::new(Device::Cpu);
    println!("Context cr√©√© pour: {:?}", context.device());

    // Test allocation m√©moire
    let size = 1024 * 1024; // 1MB
    match context.allocator().allocate(size) {
        Ok(ptr) => {
            println!("‚úì Allocation de {} MB r√©ussie", size / (1024 * 1024));

            // Test copie host->device
            let data = vec![42u8; size];
            if context.allocator().copy_from_host(ptr, &data).is_ok() {
                println!("‚úì Copie host->device r√©ussie");
            }

            // Test copie device->host
            let mut result = vec![0u8; size];
            if context
                .allocator()
                .copy_to_host(&mut result, ptr, size)
                .is_ok()
            {
                println!("‚úì Copie device->host r√©ussie");
                if result[0] == 42 {
                    println!("‚úì Donn√©es v√©rifi√©es correctes");
                }
            }

            // Cleanup
            context.allocator().deallocate(ptr);
            println!("‚úì M√©moire lib√©r√©e");
        }
        Err(e) => {
            println!("‚ùå Erreur d'allocation: {:?}", e);
        }
    }

    // === Test synchronisation ===
    println!("\n‚è±Ô∏è  Test synchronisation:");
    match synchronize() {
        Ok(_) => println!("‚úì Synchronisation du device courant r√©ussie"),
        Err(e) => println!("‚ùå Erreur de synchronisation: {:?}", e),
    }

    // === Cr√©ation de tenseurs sur diff√©rents devices ===
    println!("\nüéØ Cr√©ation de tenseurs avec device:");

    // Tenseur CPU (par d√©faut)
    let cpu_tensor = Tensor::ones(vec![2, 3], None);
    println!(
        "Tenseur CPU cr√©√© - shape: {:?}, device: {:?}",
        cpu_tensor.shape(),
        cpu_tensor.device()
    );

    // Simule la cr√©ation d'un tenseur GPU (si disponible)
    if cuda_is_available() {
        println!("\nüöÄ Simulation tenseur GPU:");
        // Dans une impl√©mentation r√©elle, on ferait:
        // let gpu_options = TensorOptions::new().device(Device::Cuda(0));
        // let gpu_tensor = Tensor::ones(vec![2, 3], Some(gpu_options));
        println!("  (Cr√©ation de tenseur GPU disponible avec Device::Cuda(0))");
    }

    // === Cas d'usage pratiques ===
    println!("\nüí° Cas d'usage pratiques:");

    // 1. S√©lection automatique du meilleur device
    let best_device = if cuda_is_available() {
        Device::Cuda(0)
    } else if metal_is_available() {
        Device::Metal(0)
    } else {
        Device::Cpu
    };
    println!("‚Ä¢ Meilleur device disponible: {}", best_device);

    // 2. Multi-GPU training simulation
    let num_gpus = device_count("cuda");
    if num_gpus > 1 {
        println!("‚Ä¢ Multi-GPU disponible: {} GPUs CUDA", num_gpus);
        for i in 0..num_gpus {
            println!("  - Device cuda:{}", i);
        }
    }

    // 3. Device memory management
    println!("\n‚Ä¢ Gestion m√©moire par device:");
    for device in &devices {
        if let Some(info) = manager.get_device_info(device) {
            let used_memory = info.total_memory - info.available_memory;
            let usage_percent = (used_memory as f64 / info.total_memory as f64) * 100.0;
            println!("  {} - Utilisation: {:.1}%", device, usage_percent);
        }
    }

    // === Patterns d'utilisation avanc√©s ===
    println!("\nüèóÔ∏è  Patterns d'utilisation avanc√©s:");

    // Device context manager pattern
    println!("‚Ä¢ Pattern context manager:");
    println!("  with device(cuda:0):");
    println!("      tensor = Tensor::randn([1000, 1000])");
    println!("      # Toutes les op√©rations sur cuda:0");

    // Transfert entre devices
    println!("\n‚Ä¢ Transfert entre devices:");
    println!("  cpu_tensor = Tensor::ones([100, 100])");
    println!("  gpu_tensor = cpu_tensor.to(Device::Cuda(0))");
    println!("  result = gpu_tensor.matmul(&other)");
    println!("  cpu_result = result.to(Device::Cpu)");

    // Op√©rations asynchrones
    println!("\n‚Ä¢ Op√©rations asynchrones:");
    println!("  stream1 = CudaStream::new()");
    println!("  stream2 = CudaStream::new()");
    println!("  # Calculs parall√®les sur diff√©rents streams");

    println!("\n‚úÖ D√©monstration Device termin√©e !");
    println!("üì¶ Fonctionnalit√©s impl√©ment√©es:");
    println!("   ‚Ä¢ Device discovery et √©num√©ration");
    println!("   ‚Ä¢ Support multi-GPU (CUDA, Metal, ROCm)");
    println!("   ‚Ä¢ Device memory management");
    println!("   ‚Ä¢ Device synchronization");
    println!("   ‚Ä¢ Allocateurs sp√©cifiques par device");
    println!("   ‚Ä¢ Context managers pour devices");
    println!("   ‚Ä¢ Pr√©paration pour calculs h√©t√©rog√®nes");
}
