// rustytorch_examples/src/memory_pool_demo.rs
// D√©monstration du syst√®me de memory pools pour optimisation

use rustytorch_core::{DType, Device};
use rustytorch_tensor::memory_pool::{
    allocate_pooled, clear_memory_pools, memory_pool_stats, MemoryPoolManager, PoolConfig,
    PoolStatistics,
};
use std::time::Instant;

pub fn run_memory_pool_demo() {
    println!("üß† D√©monstration du Memory Pool System RustyTorch\n");

    // === Configuration du pool ===
    println!("‚öôÔ∏è Configuration du memory pool:");

    let config = PoolConfig {
        max_pool_size: 256 * 1024 * 1024, // 256 MB
        max_age_seconds: 300,             // 5 minutes
        enable_defragmentation: true,
        alignment: 64, // Cache line alignment
        growth_factor: 1.5,
        bypass_small_allocs: true,        // Enable bypass for performance test
        bypass_threshold: 4096,           // 4KB threshold
    };

    println!(
        "‚Ä¢ Taille max du pool: {} MB",
        config.max_pool_size / (1024 * 1024)
    );
    println!("‚Ä¢ √Çge max des blocs: {} secondes", config.max_age_seconds);
    println!("‚Ä¢ D√©fragmentation: {}", config.enable_defragmentation);
    println!("‚Ä¢ Alignement: {} bytes", config.alignment);
    println!("‚Ä¢ Facteur de croissance: {}", config.growth_factor);

    // === Cr√©ation du manager ===
    println!("\nüìã Cr√©ation du memory pool manager:");
    let manager = MemoryPoolManager::new(config.clone());
    println!("‚úì Manager cr√©√© avec configuration personnalis√©e");

    // === Allocation basique ===
    println!("\nüîß Allocations basiques:");

    // Premi√®res allocations (cache miss)
    let start = Instant::now();
    let ptr1 = manager.allocate(1024, Device::Cpu, DType::Float32).unwrap();
    let alloc1_time = start.elapsed();
    println!("‚Ä¢ Premi√®re allocation 1KB: {:?} (cache miss)", alloc1_time);

    let ptr2 = manager.allocate(2048, Device::Cpu, DType::Float32).unwrap();
    println!("‚Ä¢ Allocation 2KB: succ√®s");

    let ptr3 = manager.allocate(4096, Device::Cpu, DType::Float32).unwrap();
    println!("‚Ä¢ Allocation 4KB: succ√®s");

    // Lib√©ration
    manager.deallocate(ptr1, 1024, Device::Cpu, DType::Float32);
    manager.deallocate(ptr2, 2048, Device::Cpu, DType::Float32);
    println!("‚Ä¢ Lib√©ration des blocs: succ√®s");

    // === Test de r√©utilisation (cache hit) ===
    println!("\n‚ôªÔ∏è Test de r√©utilisation (cache hit):");

    let start = Instant::now();
    let ptr4 = manager.allocate(1024, Device::Cpu, DType::Float32).unwrap();
    let reuse_time = start.elapsed();
    println!("‚Ä¢ R√©allocation 1KB: {:?} (cache hit)", reuse_time);

    if reuse_time < alloc1_time {
        println!("‚úì R√©utilisation plus rapide que allocation initiale!");
    }

    // === Test avec diff√©rents types ===
    println!("\nüîÄ Test avec diff√©rents devices/dtypes:");

    // CPU F32
    let cpu_f32 = manager.allocate(8192, Device::Cpu, DType::Float32).unwrap();
    println!("‚Ä¢ CPU F32 8KB: succ√®s");

    // CPU F64 (diff√©rent pool)
    let cpu_f64 = manager.allocate(8192, Device::Cpu, DType::Float64).unwrap();
    println!("‚Ä¢ CPU F64 8KB: succ√®s (pool s√©par√©)");

    // Test CUDA (si disponible - simul√© ici)
    println!("‚Ä¢ CUDA pools: disponibles pour allocation future");

    // === Statistiques en temps r√©el ===
    println!("\nüìä Statistiques du pool:");
    let stats = manager.global_statistics();
    print_pool_statistics(&stats);

    // === Test de performance ===
    println!("\nüèÉ Test de performance (1000 allocations):");

    // Sans pool (allocation syst√®me directe)
    let start = Instant::now();
    let mut system_ptrs = Vec::new();
    for _ in 0..1000 {
        let layout = std::alloc::Layout::from_size_align(1024, 64).unwrap();
        let ptr = unsafe { std::alloc::alloc(layout) };
        system_ptrs.push((ptr, layout));
    }
    let system_time = start.elapsed();

    // Nettoyage
    for (ptr, layout) in system_ptrs {
        unsafe {
            std::alloc::dealloc(ptr, layout);
        }
    }

    // Avec pool
    let start = Instant::now();
    let mut pool_ptrs = Vec::new();
    for _ in 0..1000 {
        let ptr = manager.allocate(1024, Device::Cpu, DType::Float32).unwrap();
        pool_ptrs.push(ptr);
    }
    let pool_time = start.elapsed();

    // Nettoyage
    for ptr in pool_ptrs {
        manager.deallocate(ptr, 1024, Device::Cpu, DType::Float32);
    }

    println!("‚Ä¢ Allocation syst√®me: {:?}", system_time);
    println!("‚Ä¢ Allocation pool: {:?}", pool_time);

    let speedup = system_time.as_nanos() as f64 / pool_time.as_nanos() as f64;
    println!("‚Ä¢ Acc√©l√©ration: {:.2}x", speedup);

    // === Test avec API de haut niveau ===
    println!("\nüéØ Test API haut niveau (PooledMemory):");

    {
        let pooled1 = allocate_pooled(16384, Device::Cpu, DType::Float32).unwrap();
        let pooled2 = allocate_pooled(32768, Device::Cpu, DType::Float64).unwrap();

        println!(
            "‚Ä¢ PooledMemory 16KB: ptr={:?}, size={}",
            pooled1.as_ptr(),
            pooled1.size()
        );
        println!(
            "‚Ä¢ PooledMemory 32KB: ptr={:?}, size={}",
            pooled2.as_ptr(),
            pooled2.size()
        );

        // Les blocs seront automatiquement retourn√©s au pool √† la fin du scope
        println!("‚Ä¢ Lib√©ration automatique √† la fin du scope");
    }

    println!("‚úì Blocs automatiquement retourn√©s au pool");

    // === Test de fragmentation ===
    println!("\nüß© Test anti-fragmentation:");

    // Allocation de tailles vari√©es
    let mut mixed_ptrs = Vec::new();
    let sizes = vec![512, 1024, 2048, 4096, 8192];

    for &size in &sizes {
        for _ in 0..10 {
            let ptr = manager.allocate(size, Device::Cpu, DType::Float32).unwrap();
            mixed_ptrs.push((ptr, size));
        }
    }

    println!("‚Ä¢ Allou√© {} blocs de tailles vari√©es", mixed_ptrs.len());

    // Lib√©ration partielle (cr√©er fragmentation)
    for i in (0..mixed_ptrs.len()).step_by(2) {
        let (ptr, size) = mixed_ptrs[i];
        manager.deallocate(ptr, size, Device::Cpu, DType::Float32);
    }

    println!("‚Ä¢ Lib√©r√© 50% des blocs (fragmentation cr√©√©e)");

    // Test allocation apr√®s fragmentation
    let large_ptr = manager.allocate(65536, Device::Cpu, DType::Float32);
    match large_ptr {
        Ok(_) => println!("‚úì Allocation large r√©ussie malgr√© fragmentation"),
        Err(_) => println!("‚ö† Allocation large √©chou√©e (fragmentation)"),
    }

    // === Statistiques finales ===
    println!("\nüìà Statistiques finales:");
    let final_stats = memory_pool_stats();
    print_pool_statistics(&final_stats);

    // === Patterns d'utilisation recommand√©s ===
    println!("\nüí° Patterns d'utilisation recommand√©s:");
    println!("‚Ä¢ Deep Learning:");
    println!("  - Pr√©-allouer pools pour activations/gradients");
    println!("  - R√©utiliser m√©moire entre batches");
    println!("  - S√©parer pools par device (CPU/GPU)");

    println!("\n‚Ä¢ Tenseurs temporaires:");
    println!("  - Utiliser PooledMemory pour RAII");
    println!("  - Tailles power-of-2 pour meilleure r√©utilisation");
    println!("  - Aligner sur cache lines (64 bytes)");

    println!("\n‚Ä¢ Optimisation multi-thread:");
    println!("  - Un pool par thread pour √©viter contention");
    println!("  - Lock-free allocation paths");
    println!("  - Batch deallocation");

    // === Configuration adapt√©e par cas d'usage ===
    println!("\n‚ö° Configurations optimales par cas d'usage:");

    // Configuration pour training
    println!("‚Ä¢ Deep Learning Training:");
    let training_config = PoolConfig {
        max_pool_size: 2 * 1024 * 1024 * 1024, // 2GB
        max_age_seconds: 600,                  // 10 minutes
        enable_defragmentation: true,
        alignment: 256,     // GPU alignment
        growth_factor: 2.0, // Croissance agressive
        bypass_small_allocs: true,             // Enable bypass for performance
        bypass_threshold: 1024,                // 1KB threshold
    };
    println!(
        "  - Pool size: {}GB",
        training_config.max_pool_size / (1024 * 1024 * 1024)
    );
    println!("  - Max age: {}min", training_config.max_age_seconds / 60);
    println!(
        "  - Alignment: {} bytes (GPU optimal)",
        training_config.alignment
    );

    // Configuration pour inference
    println!("\n‚Ä¢ Inference/Production:");
    let inference_config = PoolConfig {
        max_pool_size: 512 * 1024 * 1024, // 512MB
        max_age_seconds: 60,              // 1 minute
        enable_defragmentation: false,    // Latence pr√©visible
        alignment: 64,                    // CPU optimal
        growth_factor: 1.25,              // Croissance conservative
        bypass_small_allocs: true,        // Maximum performance
        bypass_threshold: 2048,           // 2KB threshold
    };
    println!(
        "  - Pool size: {}MB",
        inference_config.max_pool_size / (1024 * 1024)
    );
    println!("  - Max age: {}s", inference_config.max_age_seconds);
    println!(
        "  - Defrag: {} (latence pr√©visible)",
        inference_config.enable_defragmentation
    );

    // === M√©triques avanc√©es ===
    println!("\nüìê M√©triques avanc√©es disponibles:");
    println!("‚Ä¢ Cache hit ratio: mesure efficacit√© r√©utilisation");
    println!("‚Ä¢ Peak memory usage: dimensionnement optimal");
    println!("‚Ä¢ Allocation count: d√©tection fuites m√©moire");
    println!("‚Ä¢ Defragmentation frequency: optimisation layout");

    // === Int√©gration avec tenseurs ===
    println!("\nüîó Int√©gration future avec Tensor:");
    println!("‚Ä¢ TensorOptions::use_memory_pool: bool");
    println!("‚Ä¢ Tensor::with_pool(pool_id): Self");
    println!("‚Ä¢ Automatic pool selection par device");
    println!("‚Ä¢ Pool-aware tensor operations");

    // === Nettoyage final ===
    println!("\nüßπ Nettoyage des pools:");
    clear_memory_pools();
    println!("‚úì Tous les pools ont √©t√© vid√©s");

    let empty_stats = memory_pool_stats();
    println!("‚Ä¢ Allocations restantes: {}", empty_stats.total_allocations);

    println!("\n‚úÖ D√©monstration Memory Pool termin√©e !");
    println!("üèóÔ∏è Syst√®me impl√©ment√©:");
    println!("   ‚Ä¢ DeviceMemoryPool - pool par device/dtype");
    println!("   ‚Ä¢ MemoryPoolManager - gestion globale");
    println!("   ‚Ä¢ PooledMemory - RAII smart pointer");
    println!("   ‚Ä¢ Bucket allocation - power-of-2 sizes");
    println!("   ‚Ä¢ Anti-fragmentation - compaction/cleanup");
    println!("   ‚Ä¢ Statistics tracking - performance monitoring");
    println!("   ‚Ä¢ Configuration flexible - adaptable par use case");
    println!("   ‚Ä¢ Thread-safe - parallel access");
}

/// Helper pour afficher les statistiques de pool
fn print_pool_statistics(stats: &PoolStatistics) {
    println!("  Total allocations: {}", stats.total_allocations);
    println!("  Cache hits: {}", stats.cache_hits);
    println!("  Cache misses: {}", stats.cache_misses);

    if stats.total_allocations > 0 {
        let hit_ratio = (stats.cache_hits as f64 / stats.total_allocations as f64) * 100.0;
        println!("  Cache hit ratio: {:.1}%", hit_ratio);
    }

    println!("  Defragmentations: {}", stats.defragmentations);
    println!("  Peak memory usage: {} KB", stats.peak_memory_usage / 1024);
}

/// D√©monstration des patterns avanc√©s
pub fn demo_advanced_patterns() {
    println!("\nüöÄ Patterns avanc√©s de memory management:");

    // Pattern 1: Batch processing
    println!("\n1Ô∏è‚É£ Batch Processing Pattern:");
    let manager = MemoryPoolManager::new(PoolConfig::default());

    // Pr√©-allocation pour batch
    let batch_size = 32;
    let input_size = 1024 * 1024; // 1MB per sample

    let mut batch_ptrs = Vec::new();
    for i in 0..batch_size {
        let ptr = manager
            .allocate(input_size, Device::Cpu, DType::Float32)
            .unwrap();
        batch_ptrs.push(ptr);
        if i % 8 == 0 {
            print!(".");
        }
    }
    println!("\n‚úì Batch de {} samples pr√©-allou√©", batch_size);

    // Processing simulation
    std::thread::sleep(std::time::Duration::from_millis(10));

    // Lib√©ration batch
    for ptr in batch_ptrs {
        manager.deallocate(ptr, input_size, Device::Cpu, DType::Float32);
    }
    println!("‚úì Batch processing termin√©");

    // Pattern 2: Gradient accumulation
    println!("\n2Ô∏è‚É£ Gradient Accumulation Pattern:");

    let accumulation_steps = 4;
    let grad_size = 512 * 1024; // 512KB gradients

    // Allocation persistante pour accumulation
    let accum_ptr = manager
        .allocate(grad_size, Device::Cpu, DType::Float32)
        .unwrap();
    println!("‚úì Buffer d'accumulation allou√©");

    // Simulation accumulation
    for step in 0..accumulation_steps {
        let temp_grad = manager
            .allocate(grad_size, Device::Cpu, DType::Float32)
            .unwrap();
        // Accumulate gradients (simulated)
        std::thread::sleep(std::time::Duration::from_millis(5));
        manager.deallocate(temp_grad, grad_size, Device::Cpu, DType::Float32);
        println!("  Step {}/{} processed", step + 1, accumulation_steps);
    }

    manager.deallocate(accum_ptr, grad_size, Device::Cpu, DType::Float32);
    println!("‚úì Gradient accumulation termin√©");

    // Pattern 3: Multi-device coordination
    println!("\n3Ô∏è‚É£ Multi-Device Pattern:");

    // Allocation sur diff√©rents devices
    let cpu_ptr = manager
        .allocate(1024 * 1024, Device::Cpu, DType::Float32)
        .unwrap();
    println!("‚úì CPU allocation: 1MB");

    // Simulation transfer CPU -> GPU
    println!("‚Üí Transfer vers GPU (simul√©)");
    let gpu_ptr = manager
        .allocate(1024 * 1024, Device::Cpu, DType::Float32)
        .unwrap(); // Simulated GPU
    println!("‚úì GPU allocation: 1MB");

    // Cleanup
    manager.deallocate(cpu_ptr, 1024 * 1024, Device::Cpu, DType::Float32);
    manager.deallocate(gpu_ptr, 1024 * 1024, Device::Cpu, DType::Float32);
    println!("‚úì Multi-device cleanup termin√©");

    println!("\nüéØ Patterns demonstr√©s avec succ√®s!");
}
